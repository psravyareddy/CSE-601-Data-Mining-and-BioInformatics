# -*- coding: utf-8 -*-
"""Random Forest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AOV9Q7U_cVtwuIigOgfuxLG9lAvW312_

## Project 3 - Random Forest

Team members: Sai Hari Charan, Shravya Pentaparthi, Hemant Koti <br>

Random Forest algorithm implementation.
"""

from google.colab import drive
drive.mount('/content/drive')

import traceback 
import pandas as pd
import numpy as np
import argparse
import json
import math

import random
from random import seed
from random import randrange

pd.set_option('display.max_rows', 1000)

df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Data Mining/project3_dataset1.txt', sep='\t', header=None)

df.shape

# Convert strings to categorical values
categorical_index = []
for i in range(len(df.columns) - 1):
  if str(df.dtypes[i]) == 'object':
    categorical_index.append(i)
    df[i] = df[i].astype('category').cat.codes
  df[i] = df[i].astype(np.float)

df.head(10)

df.info()

data = df.to_numpy()
print(data)

# Code: https://machinelearningmastery.com/implement-resampling-methods-scratch-python/
def train_test_split(dataset, split=0.9):
  train = list()
  train_size = split * len(dataset)
  dataset_copy = list(dataset)
  while len(train) < train_size:
      index = randrange(len(dataset_copy))
      train.append(dataset_copy.pop(index))

  return np.asarray(train), np.asarray(dataset_copy)

# Split the dataset into K folds
# Code: https://machinelearningmastery.com/implement-resampling-methods-scratch-python/
def cross_validation_split(dataset, folds=10):
    dataset_split = list()
    dataset_copy = list(dataset)
    fold_size = int(len(dataset) / folds)
    for i in range(folds):
        fold = list()
        while len(fold) < fold_size:
            index = randrange(len(dataset_copy))
            fold.append(dataset_copy.pop(index))
        dataset_split.append(fold)
    return np.asarray(dataset_split)

class Node:
  def __init__(self, left, right, pivotcol, pivotval):
    self.left = left
    self.right = right
    self.column = pivotcol
    self.cutoff = pivotval

class DecisionTree():
  def __init__(self, train):
    self.node = self.create_node(train)

  def split_data(self, data, pivotcol, pivotval):
    left = []
    right = []
    for row in range(len(data)):
      if pivotcol in categorical_index:
        if data[row][pivotcol] == pivotval:
          left.append(data[row])
        else:
          right.append(data[row])
      else:
        if data[row][pivotcol] <= pivotval:
          left.append(data[row])
        else:
          right.append(data[row])
          
    left = np.asarray(left)
    right = np.asarray(right)
    return left, right
    
  def gini_index(self, left, right):    
    left_one = left_zero = 0
    right_one = right_zero = 0
      
    if len(left) > 0:
      left_one = float(np.sum(left[:, -1] == 1)) / len(left)
      left_zero = float(np.sum(left[:, -1] == 0)) / len(left)
    if len(right) > 0:
      right_one = float(np.sum(right[:, -1] == 1)) / len(right)
      right_zero = float(np.sum(right[:, -1] == 0)) / len(right)

    gini_index_left = 1.0 - (left_one ** 2  + left_zero ** 2)
    gini_index_right = 1.0 - (right_one ** 2 + right_zero ** 2)
      
    return (gini_index_left * len(left) + gini_index_right * len(right)) / (len(left) + len(right))

  # BST create node
  def create_node(self, data):
    min_gini = float('inf')
    left = np.array([])
    right = np.array([])
    pivotval = -1
    pivotcol = -1

    for col in random.sample(range(0, len(data[0]) - 1), num_features_for_split):
      for row in range(len(data)):
        _left, _right = self.split_data(data, col, data[row][col])
        gini = self.gini_index(_left, _right)
        if gini < min_gini:
          min_gini = gini
          left = _left
          right = _right
          pivotval = data[row][col]
          pivotcol = col
    
    return Node(left, right, pivotcol, pivotval)

  def output(self, left, right):
    zero_count = 0
    one_count = 0

    if len(left) > 0:
      zero_count += np.sum(left[:, -1] == 0)
      one_count += np.sum(left[:, -1] == 1)

    if len(right) > 0:
      zero_count += np.sum(right[:, -1] == 0)
      one_count += np.sum(right[:, -1] == 1)
      
    return 1 if one_count > zero_count else 0

  # BST traverse and append nodes to left or right
  def fit(self, node):
    left = node.left
    right = node.right

    del(node.left)
    del(node.right)

    if len(left) == 0 or len(right) == 0:
      node.left = node.right = self.output(left, right)
      return self.node

    if len(left) > 0:
      node.left = self.output(left, np.array([])) if len(np.unique(left[:,-1])) == 1 else self.fit(self.create_node(left))
    if len(right) > 0:
      node.right = self.output(np.array([]), right) if len(np.unique(right[:,-1])) == 1 else self.fit(self.create_node(right))

    return node

# BST search logic
def predict(node, test):
    
  if node == 0 or node == 1:
    return node

  if node.column in categorical_index:
    if test[node.column] == node.cutoff:
      return node.left if node.left == 0 or node.left == 1 else predict(node.left, test)
    else:
      return node.right if node.right == 0 or node.right == 1 else predict(node.right, test)
  else:
    if test[node.column] < node.cutoff:
      return node.left if node.left == 0 or node.left == 1 else predict(node.left, test)
    else:
      return node.right if node.right == 0 or node.right == 1 else predict(node.right, test)

# Code: https://machinelearningmastery.com/implement-machine-learning-algorithm-performance-metrics-scratch-python/
def metrics(actual, predicted):
  tp = fn = fp = tn = 0

  for i in range(len(actual)):
    if actual[i] == 1 and predicted[i] == 1:
      tp += 1
    elif actual[i] == 1 and predicted[i] == 0:
      fn += 1
    elif actual[i] == 0 and predicted[i] == 1:
      fp += 1
    elif actual[i] == 0 and predicted[i] == 0:
      tn += 1

  return tp, fn, fp, tn

def randomforest(train, test):
  predicted_total = []
  for tree in range(n_trees):
    sample = train[np.random.choice(len(train), len(train), replace=True), :]
    decisiontree = DecisionTree(sample)
    parent = decisiontree.fit(decisiontree.node)    

    predicted = []
    for row in range(len(test)):
      predicted.append(predict(parent, test[row]))

    predicted_total.append(np.asarray(predicted))

  predicted = []
  predicted_total = np.transpose(predicted_total)
  for row in range(len(predicted_total)):
    occurences = np.bincount(predicted_total[row])
    predicted.append(np.argmax(occurences))
    
  return predicted

"""## Random Forest using K fold cross validation for metric evaluation"""

num_features_for_split = 6
# print('Number of features: ', num_features_for_split)
n_trees = 100

accuracy = 0
precision = 0
recall = 0
f1 = 0
K = 10

seed(1)
folds = cross_validation_split(data, K)
for i in range(len(folds)):
  print(i)
  test = folds[i]
  train = np.array([])
  for k in range(len(folds)):
    if k != i:  
      train = np.vstack((train, folds[k])) if len(train) != 0 else folds[k]

  predicted = randomforest(train, test)

  tp, fn, fp, tn = metrics(test[:,-1], np.asarray(predicted))
  accuracy += float(tp + tn) / (tp + fn + fp + tn)
  
  if (tp + fp) is not 0:
    precision += float(tp) / (tp + fp)

  if (tp + fn) is not 0:
    recall += float(tp) / (tp + fn)

  if (2 * tp + fn + fp) is not 0:
    f1 += float(2 * tp) / (2 * tp + fn + fp)

print("Average Accuracy:", accuracy * K)
print("Average Precision:", precision * K)
print("Average Recall:", recall * K)
print("Average F-1 Score:", f1 * K)

"""## Random Forest using Scikit Learn"""

X = df.iloc[:, :-1]
y = df.iloc[:, -1]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = n_trees, max_features = num_features_for_split, random_state = 42)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""## References

Code
  1. https://pbpython.com/categorical-encoding.html
  2. https://machinelearningmastery.com/implement-resampling-methods-scratch-python/
  3. https://medium.com/@penggongting/implementing-decision-tree-from-scratch-in-python-c732e7c69aea
  4. https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/
  5. https://machinelearningmastery.com/implement-machine-learning-algorithm-performance-metrics-scratch-python/
  6. https://machinelearningmastery.com/implement-random-forest-scratch-python/

Readings
  1. https://www.analyticsvidhya.com/blog/2020/10/all-about-decision-tree-from-scratch-with-python-implementation/
  2. https://medium.com/@penggongting/implementing-decision-tree-from-scratch-in-python-c732e7c69aea
  3. https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/
  4. https://towardsdatascience.com/random-forests-and-decision-trees-from-scratch-in-python-3e4fa5ae4249
"""