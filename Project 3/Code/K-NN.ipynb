{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5eqaIJ1XCBR"
   },
   "source": [
    "# **CSE-601 Project-3 K-NearestNeighbor**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import sys\n",
    "from random import randrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(X_train,X_test,Y_train,k):\n",
    "\n",
    "    tx1 = X_train.copy()\n",
    "    tx2 = Y_train.copy()\n",
    "    \n",
    "    tx1['actualvalue'] = tx2 \n",
    "\n",
    "    finalans = []\n",
    "    for index,row in X_test.iterrows():\n",
    "        row = row.values.reshape(1,row.shape[0])\n",
    "        t = np.tile(row,(X_train.shape[0],1))\n",
    "        x1 = np.sqrt(np.sum((X_train - t)**2,1))\n",
    "        x1 = x1.to_frame()\n",
    "        tx1['Edistance'] = x1\n",
    "        adddist1 = tx1.sort_values('Edistance')\n",
    "        ndata = adddist1.iloc[0:k]\n",
    "        ndata2 = ndata.groupby('actualvalue')['actualvalue'].count()\n",
    "        ndata2 = ndata2.sort_values(ascending= False)\n",
    "        test =  ndata2.keys()\n",
    "        ans = test[0]\n",
    "        finalans.append(ans)\n",
    "    return finalans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(actual, predicted):\n",
    "\n",
    "    tp = fn = fp= tn = 0\n",
    "    accuracy = precision = recall = f1_measure = 0\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        if actual[i] == 1 and predicted[i] == 1:\n",
    "            tp += 1\n",
    "        elif actual[i] == 1 and predicted[i] == 0:\n",
    "            fn += 1\n",
    "        elif actual[i] == 0 and predicted[i] == 1:\n",
    "            fp += 1\n",
    "        elif actual[i] == 0 and predicted[i] == 0:\n",
    "            tn += 1\n",
    "\n",
    "    accuracy += (float(tp+tn)/(tp+fn+fp+tn))\n",
    "\n",
    "    if(tp+fp != 0):\n",
    "        precision += (float(tp)/(tp+fp))\n",
    "\n",
    "    if(tp+fn != 0):\n",
    "        recall += (float(tp)/(tp+fn))\n",
    "\n",
    "    f1_measure += (float(2*tp)/((2*tp)+fn+fp))\n",
    "\n",
    "    return accuracy, precision, recall, f1_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_split(dataset, folds=3):\n",
    "        dataset_split = list()\n",
    "        dataset_copy = list(dataset)\n",
    "        fold_size = int(len(dataset) / folds)\n",
    "        for i in range(folds):\n",
    "            fold = list()\n",
    "            while len(fold) < fold_size:\n",
    "                index = randrange(len(dataset_copy))\n",
    "                fold.append(dataset_copy.pop(index))\n",
    "            dataset_split.append(fold)\n",
    "        return np.asarray(dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(inputfile):\n",
    "    with open(inputfile) as textFile:\n",
    "        lines=[line.split(\"\\t\") for line in textFile]\n",
    "    data=np.asarray(lines)\n",
    "    \n",
    "    #Handling Categorical data\n",
    "    sample_data = data[1]\n",
    "    categoricaldata_cols=[]\n",
    "    for col in range(data.shape[1]):\n",
    "        feature = sample_data[col]\n",
    "        if feature.isalpha():\n",
    "            array,i = np.unique(data[:,col],return_inverse = True)\n",
    "            data[:,col] = i.astype(np.float)\n",
    "            categoricaldata_cols.append(col) \n",
    "            \n",
    "    points= np.matrix(data[:,:-1],dtype=float,copy=False)\n",
    "    ground_truth_labels = np.asarray(data[:,-1],dtype=int)\n",
    "            \n",
    "    #Normalising the data\n",
    "    for col in range(points.shape[1]):\n",
    "        if col not in categoricaldata_cols:\n",
    "            v = points[:,col]\n",
    "            points[:,col] = (v - v.min()) / (v.max() - v.min())\n",
    "\n",
    "    return points,ground_truth_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the file name :project3_dataset2.txt\n",
      "enter value of k in K-fold cross validation:10\n",
      "enter value of k in K-NN :10\n",
      "the mean values of metrics are :\n",
      "Accuracy 66.30434782608697\n",
      "Precision 46.06903145873734\n",
      "Recall 51.999376354639516\n",
      "F1-Measure 0.4836826271197795\n"
     ]
    }
   ],
   "source": [
    "file_name = input(\"enter the file name :\")\n",
    "\n",
    "k = int(input(\"enter value of k in K-fold cross validation:\"))\n",
    "k1 = int(input(\"enter value of k in K-NN :\"))\n",
    "\n",
    "points,ground_truth_labels=preprocess(file_name)\n",
    "\n",
    "df = pd.DataFrame(points)\n",
    "df[df.shape[1]] = ground_truth_labels\n",
    "dataset_split  =  cross_validation_split(df.to_numpy(), 10)\n",
    "dataset_split = dataset_split.tolist()\n",
    "\n",
    "res = []\n",
    "acc_sum = 0 \n",
    "prec_sum = 0\n",
    "recall_sum = 0\n",
    "f1_measure_sum = 0 \n",
    "\n",
    "for i in range(len(dataset_split)):\n",
    "    train = dataset_split[:i] + dataset_split[i+1:]\n",
    "    test = dataset_split[i]\n",
    "    test =  pd.DataFrame(test)\n",
    "    df_train =  pd.DataFrame()\n",
    "    for j in range(len(train)):\n",
    "        t =  pd.DataFrame(train[j])\n",
    "        df_train = df_train.append(t)\n",
    "    test_label=KNN(df_train.iloc[:,:-1],test.iloc[:,:-1],df_train.iloc[:,-1:],1)\n",
    "    acc, prec, recall, f1_measure = metrics(test_label,test[test.columns[-1]])\n",
    "    acc_sum = acc_sum+acc\n",
    "    prec_sum = prec_sum+prec\n",
    "    recall_sum =recall_sum+recall\n",
    "    f1_measure_sum = f1_measure_sum+f1_measure\n",
    "print(\"the mean values of metrics are :\")\n",
    "print(\"Accuracy\",(acc_sum/(k))*100)\n",
    "print(\"Precision\",(prec_sum/(k))*100)\n",
    "print(\"Recall\",(recall_sum/(k))*100)\n",
    "print(\"F1-Measure\",f1_measure_sum/(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Project2_Recitation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
